{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#     Voice Gender Classification Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell imports all necessary Python libraries for the project:\n",
    "- **NumPy** and **Pandas** for numerical operations and data handling\n",
    "- **Scikit-learn** utilities for model training, evaluation, and preprocessing\n",
    "- **Librosa** for audio feature extraction (MFCCs, spectral features)\n",
    "- **Noisereduce** for audio denoising\n",
    "- **Sounddevice** and **Scipy** for audio recording and file I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import os\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell defines a custom `GaussianNaiveBayes` classifier from scratch:\n",
    "- **Key Methods**:\n",
    "  - `fit()`: Computes class-wise mean, variance, and priors from training data.\n",
    "  - `_log_gaussian_pdf()`: Calculates log probabilities using Gaussian PDF (stable log-space implementation).\n",
    "  - `predict()`: Classifies samples by maximizing log posterior probabilities.\n",
    "- **Optimizations**: \n",
    "  - Adds `1e-9` to variances to avoid division by zero.\n",
    "  - Uses log probabilities to prevent numerical underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Return parameters - required for scikit-learn compatibility\"\"\"\n",
    "        return {}  # Return empty dict if no parameters\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set parameters - required for scikit-learn compatibility\"\"\"\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.means = {}\n",
    "        self.variances = {}\n",
    "        self.priors = {}\n",
    "\n",
    "        for current_class in self.classes:\n",
    "            X_cls = X[y == current_class]\n",
    "            self.means[current_class] = np.mean(X_cls, axis=0)\n",
    "            self.variances[current_class] = np.var(X_cls, axis=0, ddof=1) + 1e-9\n",
    "            self.priors[current_class] = len(X_cls) / len(X)\n",
    "\n",
    "    def _log_gaussian_pdf(self, x, mean, var):\n",
    "        fo8 = np.exp(- (x - mean)**2 / (2 * var))\n",
    "        t7t = np.sqrt(2 * np.pi * var)\n",
    "        # Avoid underflow by using log space calculations \n",
    "        # log(fo8) - log(t7t) = log(fo8 / t7t)\n",
    "        # This is numerically more stable than calculating pdf directly\n",
    "        return np.log(fo8) - np.log(t7t)  \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            class_log_probs = {}\n",
    "            for current_class in self.classes:\n",
    "                # Calculate log prior\n",
    "                log_prior = np.log(self.priors[current_class])\n",
    "                \n",
    "                # likelihood for each feature and sum\n",
    "                log_likelihood = np.sum(\n",
    "                    self._log_gaussian_pdf(x, self.means[current_class],  self.variances[current_class] )\n",
    "                )\n",
    "                \n",
    "                # Total log probability\n",
    "                class_log_probs[current_class] = log_prior + log_likelihood\n",
    "            \n",
    "            # Select class with highest log probability\n",
    "            predictions.append(max(class_log_probs, key=class_log_probs.get))\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`extract_features()` Function\n",
    "Processes audio files to extract discriminative features:\n",
    "1. **Preprocessing**:\n",
    "   - Noise reduction using spectral gating.\n",
    "   - Silence trimming (30dB threshold).\n",
    "   - Amplitude normalization.\n",
    "2. **Feature Extraction**:\n",
    "   - 13 MFCCs (mean across frames).\n",
    "   - Spectral centroid, rolloff, and zero-crossing rate.\n",
    "\n",
    "#### **`build_dataset()` Function\n",
    "Constructs a labeled dataset from audio files:\n",
    "- Organizes data by gender (`male=0`, `female=1`).\n",
    "- Handles missing directories gracefully.\n",
    "- Returns features and labels as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # Noise Reduction\n",
    "    # Estimate noise from a silent part (first 0.5 sec)\n",
    "    noise_sample = y[0:int(0.5 * sr)]\n",
    "    y_denoised = nr.reduce_noise(y=y, sr=sr, y_noise=noise_sample)\n",
    "\n",
    "    # Silence Removal\n",
    "    y_trimmed, _ = librosa.effects.trim(y_denoised, top_db=30)\n",
    "\n",
    "    # Normalization (scaling waveform to -1 to 1)\n",
    "    y_normalized = librosa.util.normalize(y_trimmed)\n",
    "\n",
    "    #  Feature Extraction\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=y_normalized, sr=sr, n_mfcc=13), axis=1)\n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y_normalized, sr=sr))\n",
    "    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y_normalized, sr=sr))\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y_normalized))\n",
    "    \n",
    "\n",
    "    #  Feature Vector\n",
    "    return np.concatenate([mfccs, [spectral_centroid, spectral_rolloff, zcr]])\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(folder_path):\n",
    "    print(\"Building dataset...\")\n",
    "    print(\"Folder path:\", folder_path)\n",
    "    features = []\n",
    "    labels = []\n",
    "    label_map = {'male': 0, 'female': 1}\n",
    "\n",
    "    for label_name, label_value in label_map.items():\n",
    "        subfolder = os.path.join(folder_path, label_name)\n",
    "        if not os.path.isdir(subfolder):\n",
    "            print(f\"Skipping: {subfolder} (not a folder)\")\n",
    "            continue\n",
    "        for file_name in os.listdir(subfolder):\n",
    "            if file_name.endswith('.wav'):\n",
    "                path = os.path.join(subfolder, file_name)\n",
    "                feature_vector = extract_features(path)\n",
    "                features.append(feature_vector)\n",
    "                labels.append(label_value)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "\n",
    "def save_features_to_csv(features, labels, output_file):\n",
    "    \"\"\"\n",
    "    Save extracted features and labels to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        features (np.array): Array of feature vectors\n",
    "        labels (np.array): Array of corresponding labels\n",
    "        output_file (str): Path to the output CSV file\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the features and labels\n",
    "    df = pd.DataFrame(features)\n",
    "    \n",
    "    # Add column names for MFCCs and other features\n",
    "    columns = [f'mfcc_{i}' for i in range(13)] + ['spectral_centroid', 'spectral_rolloff', 'zcr']\n",
    "    df.columns = columns\n",
    "    \n",
    "    # Add the label column\n",
    "    df['label'] = labels\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Features saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Utility Functions`\n",
    "- **`print_features_with_labels()`**:  \n",
    "  Displays formatted feature vectors with headers (MFCCs, spectral stats) and labels for debugging.\n",
    "\n",
    "- **`record_voice()`**:  \n",
    "  Captures audio via microphone (10s duration, 22.05kHz sample rate) and saves as `input.wav`.\n",
    "\n",
    "- **`evaluate_model()`**:  \n",
    "  Computes and prints classification metrics (Accuracy, Precision, Recall, F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_features_with_labels(X, y):\n",
    "    # Feature headers\n",
    "    mfcc_headers = [f\"MFCC_{i+1:02d}\" for i in range(13)]  \n",
    "    other_headers = ['Spectral_Centroid', 'Spectral_Rolloff', 'ZCR']\n",
    "    headers = mfcc_headers + other_headers + ['Label']\n",
    "    \n",
    "    separator = \"-\" * 80\n",
    "    \n",
    "    print(f\"\\n{separator}\")\n",
    "    print(f\"{'Features with Labels':^80}\")  \n",
    "    print(separator)\n",
    "    print(\"\\t\".join(f\"{h:<15}\" for h in headers))  \n",
    "    \n",
    "    for features, label in zip(X, y):\n",
    "        formatted_features = [f\"{val:>15.4f}\" for val in features] \n",
    "        formatted_label = f\"{label:>15}\"\n",
    "        print(\"\\t\".join(formatted_features + [formatted_label]))\n",
    "    \n",
    "    print(separator)\n",
    "        \n",
    "\n",
    "\n",
    "def record_voice(filename='input.wav', duration=10, fs=22050):\n",
    "    print(\"Recording...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()\n",
    "    write(filename, fs, recording)\n",
    "    print(\"Recording saved as\", filename)\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Dataset Construction**:\n",
    "   - Loads audio data from `./audio_data`.\n",
    "   - Splits into 70% train / 30% test sets with stratification.\n",
    "\n",
    "### 2. **Feature Scaling**:\n",
    "   - Standardizes features (zero mean, unit variance) using `StandardScaler`.\n",
    "\n",
    "### 3. **Model Training & Evaluation**:\n",
    "   - **Custom GNB**: Matches scikit-learn's `GaussianNB` (80% accuracy).\n",
    "   - **Logistic Regression**: Achieves 86.67% accuracy (best performer).\n",
    "   - Prints detailed feature-label pairs for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset...\n",
      "Folder path: ./audio_data\n",
      "Features saved to features.csv\n",
      "Feature shape: (100, 16)\n",
      "Label shape: (100,)\n",
      "Train set shape: (70, 16)\n",
      "Test set shape: (30, 16)\n",
      "Train labels shape: (70,)\n",
      "Test labels shape: (30,)\n",
      "\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                              Features with Labels                              \n",
      "--------------------------------------------------------------------------------\n",
      "MFCC_01        \tMFCC_02        \tMFCC_03        \tMFCC_04        \tMFCC_05        \tMFCC_06        \tMFCC_07        \tMFCC_08        \tMFCC_09        \tMFCC_10        \tMFCC_11        \tMFCC_12        \tMFCC_13        \tSpectral_Centroid\tSpectral_Rolloff\tZCR            \tLabel          \n",
      "      -318.0206\t        70.8188\t        16.9755\t        37.0357\t       -11.1962\t        -4.3173\t        -8.5741\t       -26.4434\t       -12.6614\t        -6.0836\t       -13.9412\t         2.8369\t        -6.9908\t      2335.8841\t      3772.7200\t         0.2281\t              0\n",
      "      -318.2982\t       104.2221\t        22.5472\t        35.2791\t         1.1803\t         1.0310\t        -8.8692\t       -14.0210\t       -16.0315\t         0.3180\t        -3.4227\t        -6.6063\t        -3.1765\t      1685.2174\t      2971.8223\t         0.1435\t              0\n",
      "      -329.0503\t        74.1717\t        21.8769\t        40.8288\t         5.5121\t        -4.4789\t       -24.5533\t       -22.0097\t        -8.7642\t        -8.9279\t       -12.0080\t        -3.1402\t        -8.4825\t      1870.4103\t      2977.4577\t         0.1844\t              0\n",
      "      -372.1418\t        80.7186\t        28.4006\t        28.7262\t         1.0532\t         5.7051\t       -10.2862\t        -2.0870\t        -5.5797\t         3.5248\t        -6.4856\t         2.2410\t        -5.1809\t      1930.3098\t      3074.8084\t         0.1963\t              0\n",
      "      -307.3465\t        53.0448\t        18.3828\t        58.3413\t        16.0137\t       -12.1224\t       -21.8062\t        -9.6996\t        -9.8565\t         5.4715\t       -17.9446\t         4.9016\t        -2.6397\t      1968.2859\t      3257.2181\t         0.1652\t              0\n",
      "      -286.6229\t        80.5846\t        21.2653\t        61.5590\t         6.7075\t        -2.1387\t       -17.4385\t        -6.1358\t        -6.3403\t        -6.3891\t        -3.5322\t        -3.5955\t        -1.4991\t      1815.8481\t      3131.0955\t         0.1431\t              0\n",
      "      -309.8357\t        85.3651\t        11.9690\t        31.0633\t        -7.7440\t         4.8206\t        -8.4252\t       -17.4242\t       -16.6179\t        -0.0534\t        -8.3731\t        -2.3086\t        -3.4963\t      2131.4515\t      3470.6648\t         0.2246\t              0\n",
      "      -312.6955\t        90.2419\t         4.9604\t        36.6044\t        -0.6442\t        -3.2618\t         2.8749\t        -7.3055\t        -0.9502\t        -2.1352\t        -8.7908\t         0.9061\t         2.9786\t      1787.1087\t      2903.6711\t         0.1425\t              0\n",
      "      -335.4616\t        98.2950\t         7.9572\t        35.6967\t       -17.1743\t         0.1449\t        -8.0927\t       -29.3942\t       -10.2290\t        -3.1555\t       -19.8784\t         3.6534\t        -8.4062\t      1794.8769\t      2958.3586\t         0.1652\t              0\n",
      "      -322.7655\t       100.1142\t        29.2585\t        38.9157\t         1.4914\t         1.2557\t        -2.1190\t       -18.7386\t       -13.5817\t         0.6589\t        -7.2722\t        -2.7309\t        -6.2268\t      1530.2923\t      2508.8281\t         0.1440\t              0\n",
      "      -303.1802\t        90.6594\t        24.2811\t        42.2236\t        -0.3323\t         1.5614\t       -18.3904\t       -26.3748\t        -3.2386\t        -4.4036\t       -15.0142\t        -4.8240\t        -8.7139\t      1493.6879\t      2402.6256\t         0.1186\t              0\n",
      "      -361.4859\t        99.4126\t        13.1791\t        30.4016\t        -6.0550\t        10.0711\t       -12.1899\t       -16.9299\t        -8.6952\t        -4.1842\t        -6.5115\t        -1.2706\t       -11.7199\t      1782.0239\t      2790.1309\t         0.1676\t              0\n",
      "      -343.8658\t       100.1321\t         8.3943\t        42.3585\t        -4.1673\t        -1.3148\t         7.8574\t       -12.2814\t        -2.2569\t        -1.2122\t        -6.7497\t         1.1958\t        -0.6106\t      1482.1677\t      2430.0302\t         0.1160\t              0\n",
      "      -365.9670\t        80.5423\t        27.4950\t        33.5883\t        -1.5280\t         9.6822\t        -6.5390\t        -5.9064\t        -6.1845\t         5.9409\t        -4.8391\t        -1.7793\t        -6.2648\t      2056.6327\t      3119.4196\t         0.2171\t              0\n",
      "      -325.1036\t        78.2411\t        26.6894\t        36.5505\t         4.0469\t         0.4043\t       -14.3774\t       -11.4774\t       -15.5536\t         3.4376\t       -11.9550\t         0.4288\t        -2.8632\t      2063.1644\t      3184.4460\t         0.1775\t              0\n",
      "      -311.9228\t        99.7444\t        22.1047\t        50.8642\t         3.0668\t         4.7896\t       -12.8093\t        -7.1888\t        -4.2021\t        -4.1267\t        -5.1820\t        -4.3667\t         0.6155\t      1538.2810\t      2631.3996\t         0.1056\t              0\n",
      "      -335.0463\t        93.5809\t        18.1613\t        34.9118\t        -7.1007\t        11.7081\t         3.4709\t       -18.3239\t       -16.3993\t         1.0773\t        -0.2289\t        -3.7569\t        -8.6452\t      1745.2271\t      2765.1989\t         0.1639\t              0\n",
      "      -340.6722\t        98.0483\t         6.8899\t        41.0094\t        -4.8839\t        -1.5722\t         7.2726\t       -11.8992\t        -2.3034\t        -1.3673\t        -6.3846\t         1.3553\t        -0.5501\t      1477.5380\t      2432.6087\t         0.1163\t              0\n",
      "      -324.5909\t        69.9506\t        17.2013\t        56.8791\t        -8.9480\t        -6.5545\t        -6.8849\t       -18.3281\t       -14.9427\t        -6.6415\t       -16.6869\t        -2.7193\t       -11.4371\t      2119.8204\t      3370.5249\t         0.2022\t              0\n",
      "      -454.9944\t        58.1837\t         1.7306\t         5.4966\t        15.0599\t         3.3303\t         0.2915\t        -0.8791\t        -7.9220\t        -4.5960\t         0.1948\t        -3.2646\t        -2.5003\t      6578.8784\t      9005.5941\t         0.2543\t              0\n",
      "      -450.4240\t        75.8095\t        20.3119\t        12.3984\t         0.5092\t         5.3677\t        -0.1571\t        -7.9091\t        -8.4641\t        -8.2631\t        -8.5227\t        -5.9940\t        -4.1624\t      3668.8471\t      4800.6731\t         0.1681\t              0\n",
      "      -437.7754\t        67.2024\t         5.2873\t        17.8797\t         1.0339\t        -5.3654\t         0.7780\t       -16.9718\t       -10.4091\t         3.5206\t        -5.4121\t        -5.8962\t         1.6383\t      2978.7695\t      4387.4108\t         0.1105\t              0\n",
      "      -458.8553\t        73.3228\t         6.4899\t        10.3940\t        14.4342\t         1.7608\t        -2.7501\t       -10.0861\t       -15.7084\t       -11.2241\t        -7.7850\t        -9.8060\t        -8.7780\t      2763.6998\t      3701.8714\t         0.0689\t              0\n",
      "      -467.7317\t        59.0467\t        15.8570\t        11.4041\t         3.4254\t        -8.0377\t        -9.2553\t       -11.6420\t       -11.9841\t        -9.5956\t        -8.8022\t        -6.4648\t        -3.9088\t      2123.9354\t      3050.0032\t         0.0730\t              0\n",
      "      -487.3219\t        79.2147\t        17.9744\t        14.3382\t        13.5355\t         2.1700\t        -2.4076\t        -8.3837\t       -11.6352\t        -8.7799\t        -6.5139\t        -6.4509\t        -5.2197\t      3009.6953\t      4478.6932\t         0.1147\t              0\n",
      "      -492.3579\t        47.4922\t        16.9789\t        12.7554\t         9.3881\t         1.4813\t        -4.1446\t       -10.2763\t       -10.7496\t        -6.3381\t        -4.6732\t        -4.9514\t        -4.4682\t      2091.3840\t      2948.1660\t         0.0744\t              0\n",
      "      -468.2605\t        75.7218\t        -1.0159\t         9.2076\t        17.1288\t         9.5178\t         3.1387\t        -8.8896\t       -12.6154\t       -12.6393\t       -10.6679\t        -8.8259\t       -11.7456\t      2495.1407\t      3516.0277\t         0.0938\t              0\n",
      "      -411.8593\t       106.0530\t        17.1961\t        28.9808\t         2.0828\t       -16.1203\t        -2.0601\t       -12.3576\t       -16.0733\t       -12.6984\t       -12.0414\t        -5.1941\t        -6.9803\t      3380.0812\t      4725.0965\t         0.1271\t              0\n",
      "      -486.3364\t        49.1512\t         7.3507\t        11.9011\t         6.1296\t        -3.9104\t        -4.1044\t        -8.1340\t        -7.8793\t        -5.7576\t        -6.1586\t        -2.6606\t        -0.0587\t      4201.2625\t      5389.7260\t         0.1002\t              0\n",
      "      -429.5717\t        45.9035\t        10.5690\t        10.7193\t         6.9974\t        -1.8888\t        -5.4056\t        -9.1114\t        -7.3376\t        -5.3089\t        -6.8042\t        -4.0708\t        -1.4828\t      3676.2864\t      4868.0925\t         0.0956\t              0\n",
      "      -475.9442\t        58.1360\t         7.9207\t        14.9024\t        10.5595\t        -3.4903\t        -8.0733\t       -12.5498\t        -9.4567\t        -7.0587\t        -8.5071\t        -4.6768\t        -1.8800\t      3108.5894\t      4227.3509\t         0.1008\t              0\n",
      "      -496.4915\t        54.5678\t        11.6751\t        11.2319\t         6.5309\t        -4.0043\t        -7.2460\t        -9.6926\t        -7.1303\t        -6.2689\t        -8.5748\t        -4.6469\t        -1.0059\t      3692.3620\t      5013.0137\t         0.0926\t              0\n",
      "      -452.6999\t        72.7820\t        16.1533\t         9.9528\t         5.2043\t        -1.5017\t         2.2388\t        -2.8276\t        -8.7090\t        -8.1171\t        -9.9492\t        -8.6504\t        -3.6154\t      2674.8272\t      3631.1488\t         0.0896\t              0\n",
      "      -416.4435\t        89.1361\t        17.6661\t        36.8799\t        17.2893\t        15.6012\t       -14.9307\t         0.3131\t       -18.5743\t        -5.6957\t         0.1096\t       -13.5960\t        -4.1951\t      2355.0863\t      3850.6523\t         0.1202\t              0\n",
      "      -357.9746\t       101.8691\t        15.5591\t        48.7989\t        18.8658\t        -1.0835\t       -27.8946\t         6.8763\t       -27.3444\t       -15.9652\t        -1.1661\t       -12.7919\t        -9.6084\t      2350.4064\t      3969.5856\t         0.1051\t              0\n",
      "      -388.3127\t       101.5061\t        14.2077\t        46.7113\t        12.6803\t         4.1180\t       -18.7349\t         2.0518\t       -31.0165\t        -9.5727\t         0.9907\t       -10.9830\t        -7.3618\t      2556.6407\t      4079.7776\t         0.1330\t              0\n",
      "      -300.6120\t       105.3888\t        -3.4573\t        60.2814\t        -2.2393\t         0.9410\t       -20.2805\t        16.1536\t       -28.0804\t       -13.2798\t         3.9361\t        -4.3356\t        -7.3885\t      2533.6898\t      4168.3312\t         0.1164\t              0\n",
      "      -363.1655\t       109.2372\t        -1.5891\t        40.2336\t         2.2332\t         4.9596\t       -22.7506\t         5.3880\t       -23.2603\t        -9.4307\t         1.9104\t       -11.1188\t        -4.7706\t      2621.4129\t      4142.4366\t         0.1288\t              0\n",
      "      -452.5038\t       104.2215\t        18.1026\t        34.7272\t         7.2381\t        -3.8400\t       -13.9315\t        10.6211\t       -21.5127\t       -13.2584\t         2.4458\t        -6.9734\t        -6.1361\t      2719.9531\t      4061.0695\t         0.1507\t              0\n",
      "      -397.6658\t       122.6939\t        52.6015\t        42.0203\t         8.6903\t        12.4056\t         0.0427\t        -7.9767\t       -14.0006\t        -8.9069\t        -0.8235\t         2.9671\t        -3.8355\t      1355.9894\t      2145.3125\t         0.0664\t              0\n",
      "      -442.1928\t        78.5446\t        21.8688\t        24.8733\t        -0.5941\t         1.2330\t       -11.9153\t         0.9717\t        -2.4169\t        -7.9426\t       -16.1182\t       -10.4337\t        -2.6985\t      2359.2334\t      3538.5361\t         0.1400\t              0\n",
      "      -418.6590\t       102.1296\t        19.4499\t        21.4378\t         8.0970\t        -1.5067\t        -9.0849\t         3.0137\t        -5.7676\t        -5.0251\t        -5.5358\t        -4.4278\t        -4.5589\t      1945.8907\t      3207.0990\t         0.1153\t              0\n",
      "      -399.7015\t        75.6864\t         6.2773\t        31.4204\t        17.0604\t         0.8331\t        -9.0071\t        -0.5599\t       -10.7235\t        -6.1906\t        -7.7520\t        -5.7616\t        -6.2749\t      2404.1938\t      3774.6776\t         0.1411\t              0\n",
      "      -404.6538\t        58.7258\t         3.6016\t        34.3615\t         0.1571\t         0.8344\t       -13.6218\t       -13.6041\t       -13.3600\t        -9.2524\t        -5.6059\t        -7.6172\t        -4.7139\t      2436.6197\t      3535.4925\t         0.1517\t              0\n",
      "      -423.1046\t        89.0197\t         6.3542\t        28.0087\t         0.1744\t        -2.1012\t       -16.3171\t       -16.9840\t        -9.5858\t        -7.0006\t        -6.3384\t        -9.1676\t        -8.6486\t      2161.0288\t      3135.3979\t         0.1233\t              0\n",
      "      -402.1396\t        89.8527\t         0.6922\t        31.6465\t        11.8706\t         3.1228\t       -14.6146\t       -13.8944\t       -17.1234\t       -13.7995\t        -5.6383\t        -5.3009\t        -7.0345\t      1951.1489\t      3019.9820\t         0.1117\t              0\n",
      "      -410.4342\t       112.8596\t         7.0969\t        32.7980\t        17.4628\t         6.6444\t        12.0462\t        11.6655\t        -5.9508\t       -29.7873\t        -9.7067\t        -1.5297\t       -15.6209\t      1503.3840\t      2284.2400\t         0.0787\t              0\n",
      "      -438.7421\t       114.3485\t        -1.9668\t         4.4322\t        11.2836\t         7.5200\t        -2.6117\t        -4.2383\t        -3.1737\t       -18.6465\t        -6.4464\t         3.7933\t       -11.3366\t      1778.7790\t      2707.7546\t         0.1052\t              0\n",
      "      -414.0558\t        78.8459\t         2.5001\t        26.4467\t        -5.6717\t         2.2031\t        -0.7869\t       -11.7054\t       -11.4062\t       -11.3102\t       -13.2854\t        -1.9040\t        -6.2259\t      2146.9852\t      3319.6117\t         0.1671\t              0\n",
      "      -523.7907\t        59.0856\t        13.3588\t        10.6215\t         4.9615\t        -7.3263\t        -9.6447\t       -12.3235\t       -11.0912\t        -4.2409\t        -3.1956\t        -5.4537\t        -4.1387\t      2795.5114\t      3699.5224\t         0.0896\t              0\n",
      "      -368.4330\t        65.5687\t        24.4181\t        32.4027\t        -8.9174\t       -11.6440\t       -16.4503\t       -16.3117\t        -7.7953\t        -4.2446\t        -3.6547\t        -9.1602\t         0.9751\t      2034.2494\t      3534.0278\t         0.1679\t              1\n",
      "      -350.2517\t        58.0340\t        10.2755\t        29.2279\t        -4.8290\t        -1.3033\t       -24.9551\t       -11.3035\t        -7.4163\t        -3.2297\t        -6.7749\t        -5.6806\t         2.0649\t      1965.9942\t      3300.6037\t         0.1727\t              1\n",
      "      -267.7392\t        75.0635\t        16.4507\t        24.0235\t        -7.5091\t        -3.0671\t       -10.6614\t        -5.4243\t        -2.8479\t        -8.5241\t       -12.9789\t        -6.6462\t       -11.6295\t      1790.7897\t      2933.1676\t         0.1558\t              1\n",
      "      -381.2843\t        68.7660\t        27.3807\t        24.9334\t        -7.6039\t        -7.2056\t       -21.6144\t       -10.5590\t       -17.6260\t        -2.8525\t        -5.6655\t        -8.7241\t        -2.1324\t      2204.6873\t      3448.5981\t         0.1799\t              1\n",
      "      -334.0853\t        80.9305\t        14.1745\t        34.4297\t       -14.1788\t        -1.5683\t       -23.1130\t       -13.0963\t        -7.2268\t        -6.8240\t        -4.6183\t        -3.3940\t        -1.1913\t      1955.6789\t      3098.0817\t         0.1717\t              1\n",
      "      -378.8721\t        60.3930\t        17.1840\t        32.3746\t       -10.1352\t         3.5941\t       -13.7149\t       -17.1559\t        -6.8776\t        -8.1055\t       -13.9245\t        -4.7437\t        -9.9842\t      2246.0528\t      3345.6308\t         0.2191\t              1\n",
      "      -312.1064\t        80.1595\t        11.4179\t        30.5226\t       -15.2113\t        -8.0015\t       -26.6450\t       -19.4306\t       -18.2054\t        -9.1112\t       -11.9824\t        -9.2065\t        -4.2721\t      1721.8279\t      2884.4572\t         0.1315\t              1\n",
      "      -340.5312\t        86.5113\t        15.3508\t        22.5564\t       -15.5140\t        -7.8655\t       -10.5530\t        -2.1944\t       -10.6712\t        -4.3306\t       -15.9882\t        -5.3798\t       -12.9803\t      1903.7958\t      2946.7866\t         0.1749\t              1\n",
      "      -357.6664\t        47.1064\t        10.7064\t        43.2329\t       -14.8582\t        -0.8261\t       -18.5584\t        -2.2966\t       -25.9951\t        -8.6517\t       -16.3589\t        -7.6296\t        -5.9331\t      2115.9474\t      3584.3905\t         0.1565\t              1\n",
      "      -310.0119\t        61.7353\t        26.5276\t        48.3010\t        -9.3501\t       -15.8794\t        -6.7118\t       -13.8121\t       -21.7152\t        -7.0690\t        -6.0959\t       -10.0366\t        -3.6373\t      2275.5046\t      3540.4412\t         0.1848\t              1\n",
      "      -292.6358\t        67.9643\t         3.9469\t        49.2553\t       -19.2563\t        -9.3565\t       -13.7384\t       -12.9776\t       -10.5605\t        -9.0227\t        -7.3124\t        -6.1891\t        -0.7125\t      2018.8558\t      3416.7763\t         0.1660\t              1\n",
      "      -273.2687\t        56.3155\t         7.7387\t        60.6145\t        -7.5808\t        -8.2271\t        -3.6120\t       -18.0333\t        -8.8071\t       -14.1004\t       -19.3127\t        -5.3693\t       -13.4746\t      2025.4661\t      3390.4533\t         0.1623\t              1\n",
      "      -361.8629\t        38.8910\t        14.7871\t        25.3684\t       -23.0954\t        -1.2752\t       -22.4445\t        -1.8884\t       -23.5027\t        -6.9768\t       -16.1298\t        -8.8590\t        -3.1084\t      2703.5597\t      4277.5263\t         0.2309\t              1\n",
      "      -346.5335\t        57.6252\t        34.7910\t        35.7142\t         0.2982\t        -9.1447\t       -13.2981\t        -5.3498\t       -19.1241\t        -6.1080\t        -5.8011\t        -5.0865\t        -1.4280\t      2322.3572\t      3531.5722\t         0.1968\t              1\n",
      "      -269.6684\t        60.0297\t         0.8315\t        36.9216\t       -22.2977\t        -8.5063\t       -22.6226\t       -11.0977\t       -10.5420\t        -8.2200\t        -5.0861\t        -8.4593\t        -1.1000\t      2368.0158\t      3991.7869\t         0.1883\t              1\n",
      "      -340.5933\t        50.0352\t        13.7160\t        42.7571\t        -8.7181\t         5.3252\t       -11.8778\t       -15.8700\t       -14.6961\t       -19.1879\t       -12.1765\t        -5.8318\t        -3.6111\t      2473.9450\t      3869.7630\t         0.2207\t              1\n",
      "      -328.9691\t        49.7741\t        11.2131\t        28.0001\t       -15.3588\t        -6.5297\t       -28.9174\t       -20.9779\t       -25.9664\t        -6.7497\t       -12.6896\t       -11.5576\t        -5.0443\t      2166.8217\t      3521.6686\t         0.1788\t              1\n",
      "      -308.7808\t        67.1612\t        21.1479\t        26.9030\t       -11.4878\t        -5.2803\t        -7.9985\t        -7.8466\t       -10.6405\t       -12.2731\t        -8.2837\t        -4.4624\t        -9.9653\t      2128.8675\t      3299.2560\t         0.1865\t              1\n",
      "      -238.4375\t       100.7919\t       -16.5499\t        18.8197\t       -13.6031\t        -0.5748\t       -21.2085\t       -13.2287\t       -13.3551\t        -6.0196\t        -8.1344\t        -8.5654\t        -4.8692\t      1664.4269\t      2722.7273\t         0.1340\t              1\n",
      "      -451.5479\t        64.9582\t        12.1483\t         6.8348\t         2.1807\t        -7.2005\t        -9.5975\t       -11.2954\t        -8.9012\t        -6.9849\t        -5.8428\t        -3.2954\t        -3.4369\t      2441.6189\t      3444.5126\t         0.0855\t              1\n",
      "      -466.0052\t        42.7778\t        14.5882\t         8.4735\t         3.1986\t        -3.4325\t        -6.6385\t        -8.9914\t        -6.2560\t        -2.7055\t        -2.9010\t        -2.3156\t        -1.8625\t      1538.7629\t      2246.5683\t         0.0539\t              1\n",
      "      -462.0828\t        65.6895\t        13.6405\t         4.6076\t        -0.2490\t        -5.1678\t        -7.7273\t       -13.0247\t        -9.8283\t        -6.5701\t        -7.4862\t        -4.9750\t        -3.7202\t      2512.6721\t      3455.7979\t         0.0848\t              1\n",
      "      -555.8790\t        68.1855\t        13.5215\t         9.5528\t         5.7614\t         0.2195\t        -8.1802\t       -18.4593\t       -12.0353\t        -5.9083\t        -5.9588\t        -3.7972\t        -4.9635\t      1965.9422\t      2844.6734\t         0.0651\t              1\n",
      "      -447.7448\t        56.9919\t        22.7902\t         8.8232\t        -2.0643\t        -5.8455\t        -5.6536\t       -11.2300\t       -12.2069\t        -9.2890\t        -7.6946\t        -5.9594\t        -5.8251\t      2982.6797\t      3978.2216\t         0.0970\t              1\n",
      "      -434.5940\t        53.5054\t         5.6424\t        22.5692\t         8.0088\t         1.9745\t         2.1843\t        -5.9984\t        -1.2885\t        -2.4436\t        -9.7870\t        -8.5384\t       -11.7041\t      3302.9258\t      4664.7665\t         0.1222\t              1\n",
      "      -396.1071\t        70.2968\t        14.1581\t        28.0241\t         0.8007\t        -1.7425\t         7.3502\t        -3.7913\t        -8.0154\t       -14.8448\t       -17.6607\t       -11.4018\t        -5.5417\t      2597.6213\t      3756.2538\t         0.0889\t              1\n",
      "      -426.2671\t        55.4064\t         7.2712\t        20.7931\t         8.2177\t        -6.3688\t       -11.1171\t       -12.9271\t       -12.2600\t       -15.1511\t       -12.8868\t        -8.6589\t        -9.4353\t      4528.8610\t      5806.4019\t         0.1681\t              1\n",
      "      -389.2931\t        48.1274\t         6.6166\t        18.9500\t        10.1260\t        -1.8734\t        -8.5008\t       -14.5745\t       -10.6014\t        -5.9086\t       -10.0403\t       -12.4076\t       -10.0614\t      3563.7023\t      4750.0059\t         0.1286\t              1\n",
      "      -481.7885\t        68.3563\t         3.5437\t        21.8252\t        10.1820\t        -2.3790\t        -9.7708\t       -17.3679\t       -14.6050\t        -7.7173\t        -9.5150\t        -9.7027\t       -11.5139\t      2883.0815\t      4415.7280\t         0.1039\t              1\n",
      "      -466.4910\t        58.8550\t         3.2684\t        16.0705\t         4.5597\t        -3.1959\t        -4.5586\t       -14.3656\t        -6.5249\t        -6.1572\t        -7.2216\t        -0.7682\t        -5.1819\t      2412.5220\t      3769.9962\t         0.0842\t              1\n",
      "      -570.8755\t        39.6506\t        14.5598\t         7.0786\t        -5.2405\t        -8.5404\t        -7.1125\t        -9.1559\t        -4.7570\t        -4.4041\t        -5.9074\t        -2.9036\t        -5.1652\t      2607.2083\t      3648.7623\t         0.0718\t              1\n",
      "      -431.1215\t        71.5053\t         9.6909\t         4.2764\t         6.4285\t        -4.8599\t       -14.0394\t       -21.8539\t       -19.8329\t        -8.0148\t        -3.3672\t        -6.9546\t        -7.6129\t      1902.3324\t      2763.1093\t         0.0673\t              1\n",
      "      -513.8360\t        75.4583\t        16.1864\t        20.4081\t        10.2360\t        -2.5310\t        -9.6732\t       -15.8270\t        -7.0070\t        -7.2504\t       -11.8903\t        -7.2429\t        -5.0526\t      3651.5011\t      5021.0295\t         0.1418\t              1\n",
      "      -349.9381\t        82.7596\t       -31.6275\t        27.8555\t        13.2034\t        -3.3541\t       -19.1890\t       -33.8247\t       -22.3451\t       -13.3861\t        -1.6985\t       -17.1424\t       -18.3612\t      2048.4275\t      3432.3471\t         0.1009\t              1\n",
      "      -445.6878\t        63.3886\t       -12.3227\t        14.2673\t         1.5103\t        -3.1094\t        -8.7028\t       -20.4974\t       -17.4601\t       -14.6307\t        -6.9329\t        -9.7661\t        -8.2328\t      2042.0045\t      3144.6759\t         0.1072\t              1\n",
      "      -410.9416\t        65.3025\t       -18.4331\t        28.3673\t         1.0743\t        -3.3881\t        -2.7691\t       -17.8213\t       -20.9976\t       -19.7217\t        -2.9564\t       -10.3319\t       -15.4866\t      2690.8615\t      3849.1056\t         0.1463\t              1\n",
      "      -343.3640\t        69.1115\t       -20.7015\t        21.9379\t         4.0952\t        -0.0064\t       -10.4175\t       -22.1304\t       -17.0297\t       -17.1336\t        -7.2234\t       -12.5955\t        -8.8103\t      2605.9158\t      3881.0328\t         0.1383\t              1\n",
      "      -410.1295\t        67.5100\t       -29.5600\t        20.3913\t        -0.7287\t         0.3181\t        -3.4395\t       -20.9540\t       -20.1738\t       -19.3958\t        -5.2498\t        -8.4914\t       -11.9311\t      3456.7590\t      4854.6402\t         0.1917\t              1\n",
      "      -432.1946\t        59.0619\t        -6.9637\t        21.7941\t         0.9125\t        -3.8480\t        -3.1228\t       -13.8693\t       -13.6730\t        -9.8827\t        -4.0356\t       -11.8343\t        -9.9362\t      2552.0623\t      3774.3917\t         0.1422\t              1\n",
      "      -353.1526\t        57.5245\t       -15.7951\t        30.8482\t         1.5187\t        -0.1265\t        -5.8767\t       -24.8261\t       -21.4654\t       -16.2609\t       -10.4210\t       -18.1687\t        -9.7374\t      2146.9999\t      3435.3605\t         0.1056\t              1\n",
      "      -400.7263\t        82.5987\t        -3.8985\t        29.8061\t         5.7537\t        -9.6256\t       -12.5778\t       -21.7184\t       -19.5803\t       -14.9472\t        -9.6695\t       -17.5402\t       -10.8941\t      1710.1059\t      2764.4065\t         0.0790\t              1\n",
      "      -370.0258\t        93.4585\t        -7.6009\t        33.5119\t         7.9855\t        -7.9630\t        -8.7888\t       -16.9213\t       -15.1692\t        -7.2200\t        -7.5504\t        -9.3276\t        -2.1109\t      1790.9941\t      2845.1746\t         0.1001\t              1\n",
      "      -385.6682\t        77.6779\t       -19.3045\t        48.2661\t        -4.7034\t       -15.2823\t        -5.3533\t        -7.1644\t       -14.4901\t        -2.6301\t        -9.7980\t       -15.1677\t        -4.1651\t      2443.5539\t      3512.8626\t         0.1488\t              1\n",
      "      -402.1932\t       111.4525\t         8.7213\t        26.1281\t         5.5962\t         6.5190\t        -3.6493\t       -11.6844\t       -13.9794\t       -14.1322\t        -8.3436\t        -5.9192\t        -5.1593\t      1799.2514\t      2788.5638\t         0.1067\t              1\n",
      "      -443.3503\t        61.1290\t        13.9754\t        40.4107\t         1.1224\t        22.6586\t       -22.8755\t         2.3507\t        -9.8719\t        -3.6556\t        -8.6754\t       -10.9808\t        -3.0478\t      3813.4317\t      5473.2337\t         0.2068\t              1\n",
      "      -445.9714\t        78.1543\t        -4.8186\t        22.2336\t        -2.4258\t         0.7186\t        -5.8457\t       -11.5710\t        -6.4983\t        -6.6292\t        -8.1364\t        -9.0522\t        -7.8979\t      3679.9237\t      4953.2890\t         0.2241\t              1\n",
      "      -396.9127\t       100.0953\t        13.4228\t        23.5786\t        -1.9884\t        -6.7185\t        -5.6981\t       -13.4962\t        -8.6948\t        -7.1554\t        -6.4341\t        -8.5146\t        -9.3780\t      1528.9510\t      2341.6041\t         0.0771\t              1\n",
      "      -372.9868\t        74.7561\t        18.5149\t        37.4663\t       -25.1281\t        -6.3273\t       -18.0430\t       -11.5953\t       -11.4570\t       -10.6003\t       -13.8368\t        -0.1629\t        -8.9936\t      2551.9106\t      3741.2655\t         0.1257\t              1\n",
      "      -386.0685\t        89.2683\t        -3.3891\t        23.5164\t        -0.3172\t         7.3079\t       -12.6740\t       -12.1415\t       -15.3089\t        -7.2874\t        -4.3813\t        -0.9644\t        -6.0487\t      2455.6939\t      3626.2376\t         0.1288\t              1\n",
      "      -428.9659\t        67.0431\t        13.8805\t        11.3412\t         5.6872\t        -2.9121\t        -8.4973\t       -13.9946\t        -9.5014\t        -8.8282\t        -9.3233\t        -7.1973\t       -10.3025\t      2133.7520\t      3127.2379\t         0.0797\t              1\n",
      "--------------------------------------------------------------------------------\n",
      "From Scratch Model\n",
      "Accuracy: 0.8\n",
      "Precision: 0.8\n",
      "Recall: 0.8\n",
      "F1 Score: 0.8\n",
      "Sklearn Model\n",
      "Accuracy: 0.8\n",
      "Precision: 0.8\n",
      "Recall: 0.8\n",
      "F1 Score: 0.8\n",
      "Logistic Regression Model\n",
      "Accuracy: 0.8666666666666667\n",
      "Precision: 0.9230769230769231\n",
      "Recall: 0.8\n",
      "F1 Score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X, y = build_dataset(\"./audio_data\")  \n",
    "    \n",
    "    # save_features_to_csv(X, y, \"features.csv\")\n",
    "    \n",
    "    # Load features from CSV if needed\n",
    "    # df = pd.read_csv('features.csv')\n",
    "    # X = df.drop('label', axis=1).values\n",
    "    # y = df['label'].values\n",
    "    \n",
    "    print (\"Feature shape:\", X.shape)\n",
    "    # # print first 10 samples of features\n",
    "    # print (\"First 10 samples of features:\", X[:10])\n",
    "    # # print first 10 samples of labels\n",
    "    print (\"Label shape:\", y.shape)\n",
    "    # print (\"First 10 samples of labels:\", y[:20])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=15)  #\n",
    "    \n",
    "    print (\"Train set shape:\", X_train.shape)\n",
    "    print (\"Test set shape:\", X_test.shape)\n",
    "    print (\"Train labels shape:\", y_train.shape)\n",
    "    print (\"Test labels shape:\", y_test.shape)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    \n",
    "    print_features_with_labels(X, y)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "        # My Model\n",
    "    model = GaussianNaiveBayes()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(\"From Scratch Model\")\n",
    "    evaluate_model(y_test, y_pred)\n",
    "    \n",
    "\n",
    "         # Sklearn Model \n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train_scaled, y_train)\n",
    "    sklearn_preds = gnb.predict(X_test_scaled)\n",
    "    print(\"Sklearn Model\")\n",
    "    evaluate_model(y_test, sklearn_preds)\n",
    "\n",
    "        # Logistic Regression Model\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    logistic_model = LogisticRegression()\n",
    "    logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    logistic_preds = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "    print(\"Logistic Regression Model\")\n",
    "    evaluate_model(y_test, logistic_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Live Demo**:\n",
    "   - Records audio → extracts features → scales them.\n",
    "   - Predicts gender using all trained models (Custom NB, scikit-learn NB, Logistic Regression).\n",
    "\n",
    "### 2. **Output**:\n",
    "   - Displays extracted feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features from input.wav: [[-0.34639775  1.41266773 -0.19444673  0.17144206  0.06786369 -1.29168165\n",
      "   0.40729831 -0.05474055 -0.94623946 -0.58338589 -0.52048584  0.48419222\n",
      "   0.79416779 -0.82338409 -1.02862825  0.20687238]]\n",
      "\n",
      "==================================================\n",
      "         VOICE GENDER PREDICTION RESULTS          \n",
      "==================================================\n",
      "\n",
      "Custom Model Prediction:  Male\n",
      "scikit-learn Prediction:  Male\n",
      "logistic  Prediction:     Male\n"
     ]
    }
   ],
   "source": [
    "# record_voice()\n",
    "# new_features = extract_features(\"AUD-20240503-WA0030.wav\")\n",
    "\n",
    "new_features = extract_features(\"input.wav\")\n",
    "\n",
    "new_features_scaled = scaler.transform([new_features])\n",
    "\n",
    "print(\"Extracted features from input.wav:\", new_features_scaled)\n",
    "\n",
    "custom_pred = model.predict(new_features_scaled)[0]\n",
    "sklearn_pred = gnb.predict(new_features_scaled)[0]\n",
    "logistic_model_pred = logistic_model.predict(new_features_scaled)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VOICE GENDER PREDICTION RESULTS\".center(50))\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n{'Custom Model Prediction:':<25}\", \"Male\" if custom_pred == 0 else \"Female\")\n",
    "print(f\"{'scikit-learn Prediction:':<25}\", \"Male\" if sklearn_pred == 0 else \"Female\")\n",
    "print(f\"{'logistic  Prediction:':<25}\", \"Male\" if logistic_model_pred == 0 else \"Female\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluates ensemble performance using scikit-learn's `BaggingClassifier`:\n",
    "1. **Custom NB Ensemble**: 80% accuracy.\n",
    "2. **GaussianNB Ensemble**: 83.33% accuracy.\n",
    "3. **Logistic Regression Ensemble**: 86.67% accuracy (best F1-score: 0.846).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Scikit-learn Bagging Ensemble Evaluation     \n",
      "Scikit-learn Bagging with Custom Naïve Bayes:\n",
      "Accuracy: 0.8\n",
      "Precision: 0.8461538461538461\n",
      "Recall: 0.7333333333333333\n",
      "F1 Score: 0.7857142857142857\n",
      "\n",
      "Scikit-learn Bagging with GaussianNB:\n",
      "Accuracy: 0.8333333333333334\n",
      "Precision: 0.8571428571428571\n",
      "Recall: 0.8\n",
      "F1 Score: 0.8275862068965517\n",
      "\n",
      "Scikit-learn Bagging with Logistic Regression:\n",
      "Accuracy: 0.8666666666666667\n",
      "Precision: 1.0\n",
      "Recall: 0.7333333333333333\n",
      "F1 Score: 0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Scikit-learn Bagging Ensemble Evaluation\".center(50))\n",
    "\n",
    "print(\"Scikit-learn Bagging with Custom Naïve Bayes:\")\n",
    "bagging_custom_nb = BaggingClassifier(\n",
    "    estimator=GaussianNaiveBayes(),  \n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_custom_nb.fit(X_train_scaled, y_train)\n",
    "bagging_custom_nb_preds = bagging_custom_nb.predict(X_test_scaled)\n",
    "evaluate_model(y_test, bagging_custom_nb_preds)\n",
    "\n",
    "print(\"\\nScikit-learn Bagging with GaussianNB:\")\n",
    "bagging_sk_nb = BaggingClassifier(\n",
    "    estimator=GaussianNB(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_sk_nb.fit(X_train_scaled, y_train)\n",
    "bagging_sk_nb_preds = bagging_sk_nb.predict(X_test_scaled)\n",
    "evaluate_model(y_test, bagging_sk_nb_preds)\n",
    "\n",
    "print(\"\\nScikit-learn Bagging with Logistic Regression:\")\n",
    "bagging_lr = BaggingClassifier(\n",
    "    estimator=LogisticRegression(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_lr.fit(X_train_scaled, y_train)\n",
    "bagging_lr_preds = bagging_lr.predict(X_test_scaled)\n",
    "evaluate_model(y_test, bagging_lr_preds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Combines predictions from all bagging models via `scipy.stats.mode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
      "[1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
      "[1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0]\n",
      "[1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
      "(30,)\n",
      "Ensemble Model Evaluation\n",
      "Accuracy: 0.8333333333333334\n",
      "Precision: 0.8571428571428571\n",
      "Recall: 0.8\n",
      "F1 Score: 0.8275862068965517\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "print (bagging_custom_nb_preds)\n",
    "print (bagging_sk_nb_preds)\n",
    "print (bagging_lr_preds)\n",
    "\n",
    "y = np.column_stack((bagging_custom_nb_preds, bagging_sk_nb_preds, bagging_lr_preds))\n",
    "\n",
    "\n",
    "final_y_predict = mode(y, axis=1).mode.flatten()\n",
    "print(final_y_predict)\n",
    "\n",
    "print (final_y_predict.shape)\n",
    "print(\"Ensemble Model Evaluation\")\n",
    "evaluate_model(y_test, final_y_predict)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "             Model Comparison Summary             \n",
      "==================================================\n",
      "\n",
      "Model                     Accuracy   Precision  Recall     F1        \n",
      "-----------------------------------------------------------------\n",
      "Custom NB                 0.8000     0.8000     0.8000     0.8000    \n",
      "Sklearn NB                0.8000     0.8000     0.8000     0.8000    \n",
      "Logistic Reg              0.8667     0.9231     0.8000     0.8571    \n",
      "Bagging Custom NB         0.8000     0.8462     0.7333     0.7857    \n",
      "Bagging Sklearn NB        0.8333     0.8571     0.8000     0.8276    \n",
      "Bagging Logistic Reg      0.8667     1.0000     0.7333     0.8462    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model Comparison Summary\".center(50))\n",
    "print(\"=\"*50)\n",
    "\n",
    "models = {\n",
    "    \"Custom NB\": y_pred,\n",
    "    \"Sklearn NB\": sklearn_preds,\n",
    "    \"Logistic Reg\": logistic_preds,\n",
    "    \"Bagging Custom NB\": bagging_custom_nb_preds,\n",
    "    \"Bagging Sklearn NB\": bagging_sk_nb_preds,\n",
    "    \"Bagging Logistic Reg\": bagging_lr_preds\n",
    "}\n",
    "\n",
    "print(\"\\n{:<25} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "    \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"))\n",
    "print(\"-\"*65)\n",
    "\n",
    "for name, preds in models.items():\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    prec = precision_score(y_test, preds)\n",
    "    rec = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    \n",
    "    print(\"{:<25} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
    "        name, acc, prec, rec, f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrates end-to-end gender prediction on a new recording:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "         VOICE GENDER PREDICTION RESULTS          \n",
      "==================================================\n",
      "\n",
      "Custom Model Prediction:  Male\n",
      "scikit-learn Prediction:  Male\n",
      "Logistic Regression:      Male\n",
      "Bagging Ensemble:         Male\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ensemble_pred = bagging_lr.predict(new_features_scaled)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VOICE GENDER PREDICTION RESULTS\".center(50))\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n{'Custom Model Prediction:':<25}\", \"Male\" if custom_pred == 0 else \"Female\")\n",
    "print(f\"{'scikit-learn Prediction:':<25}\", \"Male\" if sklearn_pred == 0 else \"Female\")\n",
    "print(f\"{'Logistic Regression:':<25}\", \"Male\" if logistic_model_pred == 0 else \"Female\")\n",
    "print(f\"{'Bagging Ensemble:':<25}\", \"Male\" if ensemble_pred == 0 else \"Female\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
